{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3c4f6a2-4647-49db-a9e1-39d203b5e0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.41.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: peft in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.17.1)\n",
      "Requirement already satisfied: datasets in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.4.1)\n",
      "Collecting torch==2.4.0\n",
      "  Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: torchvision==0.19.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.19.0)\n",
      "Requirement already satisfied: accelerate in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: sentence-transformers==3.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: faiss-cpu==1.8.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: pandas==2.2.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: trl in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.25.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.48.2)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch==2.4.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch==2.4.0) (4.15.0)\n",
      "Requirement already satisfied: sympy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch==2.4.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch==2.4.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch==2.4.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch==2.4.0) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch==2.4.0) (80.9.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torchvision==0.19.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torchvision==0.19.0) (12.0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sentence-transformers==3.0.1) (1.3.2)\n",
      "Requirement already satisfied: scipy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sentence-transformers==3.0.1) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sentence-transformers==3.0.1) (0.36.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.8.93)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==3.0.1) (1.2.0)\n",
      "Requirement already satisfied: psutil in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from peft) (7.1.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch==2.4.0) (3.0.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn->sentence-transformers==3.0.1) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn->sentence-transformers==3.0.1) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m212.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m257.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m252.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m139.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m178.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m256.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m265.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m264.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m265.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m177.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m155.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: triton\n",
      "\u001b[2K    Found existing installation: triton 3.5.0\n",
      "\u001b[2K    Uninstalling triton-3.5.0:\n",
      "\u001b[2K      Successfully uninstalled triton-3.5.0\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 0/15\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.8.9015\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.8.90:â”â”\u001b[0m \u001b[32m 0/15\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.8.900/15\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12â”â”â”â”\u001b[0m \u001b[32m 0/15\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.27.5/15\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.27.5:â”â”â”\u001b[0m \u001b[32m 0/15\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.27.5 0/15\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/15\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.8.93[0m \u001b[32m 2/15\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.8.93:â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/15\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93â”\u001b[0m \u001b[32m 2/15\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/15\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.9.90â”\u001b[0m \u001b[32m 3/15\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.9.90:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/15\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.9.90â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/15\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/15\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.3.83â”â”\u001b[0m \u001b[32m 4/15\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.3.83:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/15\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83â”â”â”â”\u001b[0m \u001b[32m 4/15\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/15\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90m \u001b[32m 5/15\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/15\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90[0m \u001b[32m 5/15\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/15\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93[0m \u001b[32m 6/15\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/15\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93â”\u001b[0m \u001b[32m 6/15\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/15\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90[0m \u001b[32m 7/15\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/15\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90â”\u001b[0m \u001b[32m 7/15\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/15\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.8.4.1â”â”\u001b[0m \u001b[32m 8/15\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.8.4.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/15\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1â”â”â”â”\u001b[0m \u001b[32m 8/15\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu120m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 9/15\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.3.90[0m \u001b[32m 9/15\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.3.90:â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 9/15\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90â”\u001b[0m \u001b[32m 9/15\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu121mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/15\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.10.2.21â”â”\u001b[0m \u001b[32m10/15\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.10.2.21:[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/15\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21â”â”â”â”\u001b[0m \u001b[32m10/15\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K  Attempting uninstall: torchâ”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/15\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.9.0[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/15\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.9.0:â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m12/15\u001b[0m [torch]nn-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.9.00m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: tokenizersâ”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.19.1\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n",
      "\u001b[2K    Uninstalling tokenizers-0.19.1:â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.19.10m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: transformers\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n",
      "\u001b[2K    Found existing installation: transformers 4.41.290mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n",
      "\u001b[2K    Uninstalling transformers-4.41.2:â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”\u001b[0m \u001b[32m14/15\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.41.2[90mâ•º\u001b[0m\u001b[90mâ”â”\u001b[0m \u001b[32m14/15\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15/15\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 tokenizers-0.22.1 torch-2.4.0 transformers-4.57.1 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \\\n",
    "    transformers \\\n",
    "    peft \\\n",
    "    datasets \\\n",
    "    torch==2.4.0 \\\n",
    "    torchvision==0.19.0 \\\n",
    "    accelerate \\\n",
    "    sentence-transformers==3.0.1 \\\n",
    "    faiss-cpu==1.8.0 \\\n",
    "    pandas==2.2.2 \\\n",
    "    tqdm \\\n",
    "    trl \\\n",
    "    bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe7c2c38-d8fb-41dc-aa4e-c80d2797e392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: peft 0.13.0\n",
      "Uninstalling peft-0.13.0:\n",
      "  Successfully uninstalled peft-0.13.0\n",
      "Found existing installation: transformers 4.45.0\n",
      "Uninstalling transformers-4.45.0:\n",
      "  Successfully uninstalled transformers-4.45.0\n",
      "Found existing installation: accelerate 1.11.0\n",
      "Uninstalling accelerate-1.11.0:\n",
      "  Successfully uninstalled accelerate-1.11.0\n",
      "Found existing installation: bitsandbytes 0.48.2\n",
      "Uninstalling bitsandbytes-0.48.2:\n",
      "  Successfully uninstalled bitsandbytes-0.48.2\n",
      "Collecting transformers==4.45.0\n",
      "  Using cached transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting peft==0.13.0\n",
      "  Using cached peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate==0.34.0\n",
      "  Downloading accelerate-0.34.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes==0.44.0\n",
      "  Downloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.45.0) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.45.0) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.45.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.45.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.45.0) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.45.0) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.45.0) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.45.0) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.45.0) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.45.0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from peft==0.13.0) (7.1.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from peft==0.13.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0) (1.2.0)\n",
      "Requirement already satisfied: networkx in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.0) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.13.0) (12.8.93)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.13.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft==0.13.0) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers==4.45.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers==4.45.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers==4.45.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers==4.45.0) (2025.10.5)\n",
      "Using cached transformers-4.45.0-py3-none-any.whl (9.9 MB)\n",
      "Using cached peft-0.13.0-py3-none-any.whl (322 kB)\n",
      "Downloading accelerate-0.34.0-py3-none-any.whl (324 kB)\n",
      "Downloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers, bitsandbytes, accelerate, peft\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4/4\u001b[0m [peft][32m3/4\u001b[0m [peft]erate]s]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "trl 0.25.0 requires accelerate>=1.4.0, but you have accelerate 0.34.0 which is incompatible.\n",
      "trl 0.25.0 requires transformers>=4.56.1, but you have transformers 4.45.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.34.0 bitsandbytes-0.44.0 peft-0.13.0 transformers-4.45.0\n"
     ]
    }
   ],
   "source": [
    "# Remove everything\n",
    "!pip uninstall -y peft transformers accelerate bitsandbytes\n",
    "\n",
    "# Install compatible versions\n",
    "!pip install transformers==4.45.0 peft==0.13.0 accelerate==0.34.0 bitsandbytes==0.44.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8336195",
   "metadata": {},
   "source": [
    "## **fine tune model load from huggingface and inference now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312ebaa2-6640-4d93-8520-0256d1975761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.45.0\n",
      "PEFT: 0.13.0\n",
      "Downloading and fixing adapter config...\n",
      "Original config keys: ['alpha_pattern', 'auto_mapping', 'base_model_name_or_path', 'bias', 'corda_config', 'eva_config', 'exclude_modules', 'fan_in_fan_out', 'inference_mode', 'init_lora_weights', 'layer_replication', 'layers_pattern', 'layers_to_transform', 'loftq_config', 'lora_alpha', 'lora_bias', 'lora_dropout', 'megatron_config', 'megatron_core', 'modules_to_save', 'peft_type', 'qalora_group_size', 'r', 'rank_pattern', 'revision', 'target_modules', 'target_parameters', 'task_type', 'trainable_token_indices', 'use_dora', 'use_qalora', 'use_rslora']\n",
      "Config fixed!\n",
      "Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fbf8613d38455aa37d835aa4e9ef99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully!\n",
      "\n",
      "============================================================\n",
      "Testing prediction...\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ MBTI Type: INTJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, LoraConfig\n",
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# Check versions\n",
    "import transformers, peft\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"PEFT: {peft.__version__}\")\n",
    "\n",
    "# Model paths\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model_path = \"alam1n/phi3-mbti-lora\"\n",
    "\n",
    "# Step 1: Download and fix the config file\n",
    "print(\"Downloading and fixing adapter config...\")\n",
    "config_file = hf_hub_download(repo_id=model_path, filename=\"adapter_config.json\")\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config_data = json.load(f)\n",
    "\n",
    "print(f\"Original config keys: {list(config_data.keys())}\")\n",
    "\n",
    "# Create clean config - keep only what PEFT 0.13.0 understands\n",
    "clean_config = {\n",
    "    \"base_model_name_or_path\": config_data.get(\"base_model_name_or_path\", model_name),\n",
    "    \"bias\": config_data.get(\"bias\", \"none\"),\n",
    "    \"fan_in_fan_out\": config_data.get(\"fan_in_fan_out\", False),\n",
    "    \"inference_mode\": True,\n",
    "    \"init_lora_weights\": config_data.get(\"init_lora_weights\", True),\n",
    "    \"lora_alpha\": config_data.get(\"lora_alpha\", 32),\n",
    "    \"lora_dropout\": config_data.get(\"lora_dropout\", 0.05),\n",
    "    \"modules_to_save\": config_data.get(\"modules_to_save\"),\n",
    "    \"peft_type\": \"LORA\",\n",
    "    \"r\": config_data.get(\"r\", 16),\n",
    "    \"target_modules\": config_data.get(\"target_modules\", []),\n",
    "    \"task_type\": config_data.get(\"task_type\", \"CAUSAL_LM\")\n",
    "}\n",
    "\n",
    "# Overwrite the config file\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(clean_config, f, indent=2)\n",
    "\n",
    "print(\"Config fixed!\")\n",
    "\n",
    "# Step 2: Load model with quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(model, model_path)\n",
    "\n",
    "# Step 3: Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "\n",
    "# Get special tokens\n",
    "end_token_id = tokenizer.convert_tokens_to_ids([\"<|end|>\"])[0]\n",
    "\n",
    "# Inference function\n",
    "def predict_mbti(person_text):\n",
    "    model.eval()\n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are an expert in MBTI personality analysis. Analyze the person's writing and behavior to determine their exact 4-letter MBTI type (e.g., INTJ, ESFP). Return ONLY the type.<|end|>\n",
    "<|user|>\n",
    "Analyze this person's posts and determine their MBTI type:\n",
    "\n",
    "\"{person_text}\"<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            eos_token_id=end_token_id,\n",
    "            pad_token_id=end_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[:, input_ids[\"input_ids\"].shape[-1]:][0], skip_special_tokens=False)\n",
    "    return generated_text.split(\"<|end|>\")[0].strip()\n",
    "\n",
    "# Test\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing prediction...\")\n",
    "print(\"=\"*60)\n",
    "result = predict_mbti(\"I love analyzing complex systems and finding elegant solutions. Spent the weekend refactoring code.\")\n",
    "print(f\"\\nğŸ¯ MBTI Type: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4bb6e7",
   "metadata": {},
   "source": [
    "## **Real data output Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efc4d4f-f4ad-4098-ab70-1809bb7f13bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTP\n"
     ]
    }
   ],
   "source": [
    "person_1 = \"\"\"\n",
    "I spend most of my time analyzing complex systems and optimizing architectures.\n",
    "Love diving deep into technical documentation and building efficient solutions.\n",
    "Not big on meetings unless they're focused and productive. Prefer written communication.\n",
    "Currently working on a multi-threading optimization project. The challenge is fascinating.\n",
    "Weekend plans? Probably reading research papers on distributed systems.\"\"\"\n",
    "\n",
    "result = predict_mbti(person_1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10829f22-945e-4a64-9ded-82cfbd39e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENFP\n"
     ]
    }
   ],
   "source": [
    "person_2 = \"\"\"\n",
    "Just had the most amazing brainstorming session with the team! So many creative ideas!\n",
    "Love connecting with people and exploring new possibilities. Every project is an adventure!\n",
    "Can't wait to share our new product concept at the conference next week.\n",
    "Also organizing a team outing - thinking escape room or something interactive and fun!\n",
    "Life is too short to not follow your passions and inspire others along the way!\"\"\"\n",
    "\n",
    "result = predict_mbti(person_2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d95bc5e-e59c-4b39-960d-d3bbf79582fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTP\n"
     ]
    }
   ],
   "source": [
    "person_3 = \"\"\"\n",
    "Completed the quarterly compliance audit ahead of schedule. All documentation filed properly.\n",
    "I believe in doing things right the first time. Follow procedures, meet deadlines, deliver quality.\n",
    "Updated our standard operating procedures based on last month's review.\n",
    "Maintaining our project tracking system and ensuring everyone follows the workflow.\n",
    "Consistency and attention to detail are key to success in any organization.\n",
    "\"\"\"\n",
    "\n",
    "result = predict_mbti(person_3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3113636a-a129-4ca6-b763-58cdcf075bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENFP\n"
     ]
    }
   ],
   "source": [
    "person_4 = \"\"\"\n",
    "Today was AMAZING! Closed three deals before lunch and celebrated with the team!\n",
    "Love the energy of working with people - every client interaction is unique and exciting!\n",
    "Hosting our department's happy hour this Friday. Already planning some fun activities!\n",
    "Life's about living in the moment and making every day count. Work hard, play harder!\n",
    "Just booked tickets for a spontaneous weekend trip. Who's coming with me?!\n",
    "\"\"\"\n",
    "\n",
    "result = predict_mbti(person_4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dd72a4b-b0bf-43ac-a63e-6edc84c52b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENFP\n"
     ]
    }
   ],
   "source": [
    "person_5 = \"\"\"\n",
    "Been thinking about how our work impacts people's lives and how we can do more good.\n",
    "Had a deep conversation with a colleague about creating meaningful change in our industry.\n",
    "Working on a mentorship program to help junior team members find their purpose.\n",
    "I believe technology should serve humanity and create a better future for everyone.\n",
    "Sometimes I need quiet time to reflect and recharge, but I'm always thinking of others.\n",
    "\"\"\"\n",
    "\n",
    "result = predict_mbti(person_5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aba53ccb-f29e-4477-a106-961fff7cd547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MBTI PREDICTION TEST SUITE\n",
      "======================================================================\n",
      "\n",
      "ğŸ§‘ Strategic Analyst\n",
      "ğŸ¯ Predicted Type: INTP\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ§‘ Enthusiastic Communicator\n",
      "ğŸ¯ Predicted Type: ENFP\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ§‘ Reliable Organizer\n",
      "ğŸ¯ Predicted Type: ENTP\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ§‘ Energetic Performer\n",
      "ğŸ¯ Predicted Type: ENFP\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ§‘ Insightful Idealist\n",
      "ğŸ¯ Predicted Type: ENFP\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ§‘ Innovative Debater\n",
      "ğŸ¯ Predicted Type: ENTP\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ§‘ Supportive Helper\n",
      "ğŸ¯ Predicted Type: ENTP\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ§‘ Action-Oriented Problem Solver\n",
      "ğŸ¯ Predicted Type: ENTP\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "INTERACTIVE MODE\n",
      "======================================================================\n",
      "\n",
      "Paste your text below (or type 'exit' to quit):\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your text:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Test Case 1: INTJ - Strategic Analyst\n",
    "person_1 = \"\"\"\n",
    "I spend most of my time analyzing complex systems and optimizing architectures.\n",
    "Love diving deep into technical documentation and building efficient solutions.\n",
    "Not big on meetings unless they're focused and productive. Prefer written communication.\n",
    "Currently working on a multi-threading optimization project. The challenge is fascinating.\n",
    "Weekend plans? Probably reading research papers on distributed systems.\n",
    "\"\"\"\n",
    "\n",
    "# Test Case 2: ENFP - Enthusiastic Communicator\n",
    "person_2 = \"\"\"\n",
    "Just had the most amazing brainstorming session with the team! So many creative ideas!\n",
    "Love connecting with people and exploring new possibilities. Every project is an adventure!\n",
    "Can't wait to share our new product concept at the conference next week.\n",
    "Also organizing a team outing - thinking escape room or something interactive and fun!\n",
    "Life is too short to not follow your passions and inspire others along the way!\n",
    "\"\"\"\n",
    "\n",
    "# Test Case 3: ISTJ - Reliable Organizer\n",
    "person_3 = \"\"\"\n",
    "Completed the quarterly compliance audit ahead of schedule. All documentation filed properly.\n",
    "I believe in doing things right the first time. Follow procedures, meet deadlines, deliver quality.\n",
    "Updated our standard operating procedures based on last month's review.\n",
    "Maintaining our project tracking system and ensuring everyone follows the workflow.\n",
    "Consistency and attention to detail are key to success in any organization.\n",
    "\"\"\"\n",
    "\n",
    "# Test Case 4: ESFP - Energetic Performer\n",
    "person_4 = \"\"\"\n",
    "Today was AMAZING! Closed three deals before lunch and celebrated with the team!\n",
    "Love the energy of working with people - every client interaction is unique and exciting!\n",
    "Hosting our department's happy hour this Friday. Already planning some fun activities!\n",
    "Life's about living in the moment and making every day count. Work hard, play harder!\n",
    "Just booked tickets for a spontaneous weekend trip. Who's coming with me?!\n",
    "\"\"\"\n",
    "\n",
    "# Test Case 5: INFJ - Insightful Idealist\n",
    "person_5 = \"\"\"\n",
    "Been thinking about how our work impacts people's lives and how we can do more good.\n",
    "Had a deep conversation with a colleague about creating meaningful change in our industry.\n",
    "Working on a mentorship program to help junior team members find their purpose.\n",
    "I believe technology should serve humanity and create a better future for everyone.\n",
    "Sometimes I need quiet time to reflect and recharge, but I'm always thinking of others.\n",
    "\"\"\"\n",
    "\n",
    "# Test Case 6: ENTP - Innovative Debater\n",
    "person_6 = \"\"\"\n",
    "Just challenged our entire approach in the strategy meeting. Got everyone thinking differently!\n",
    "Why do it the conventional way when we can innovate and disrupt the whole process?\n",
    "Spent the evening exploring three different solutions to the same problem. All viable!\n",
    "Love a good intellectual debate - it sharpens ideas and reveals better alternatives.\n",
    "Starting a side project that combines AI with blockchain. High risk, high reward!\n",
    "\"\"\"\n",
    "\n",
    "# Test Case 7: ISFJ - Supportive Helper\n",
    "person_7 = \"\"\"\n",
    "Made sure everyone on the team had what they needed for the presentation today.\n",
    "I find satisfaction in supporting others and making sure things run smoothly.\n",
    "Organized the welcome kit for our new hires - want them to feel comfortable here.\n",
    "Following up on customer feedback to ensure we're meeting their expectations.\n",
    "Small gestures matter. Brought coffee for the team working late on the project.\n",
    "\"\"\"\n",
    "\n",
    "# Test Case 8: ESTP - Action-Oriented Problem Solver\n",
    "person_8 = \"\"\"\n",
    "Crisis in production? I'm on it. Fixed the server issue in 20 minutes while everyone panicked.\n",
    "Don't overthink it - take action, solve problems, move forward. That's my motto.\n",
    "Negotiated a great deal with the vendor this morning. Quick thinking pays off!\n",
    "Leading the emergency response team. We handle pressure and deliver results fast.\n",
    "Life's too short for endless planning. Sometimes you just gotta do it and figure it out!\n",
    "\"\"\"\n",
    "\n",
    "# Run all tests\n",
    "test_cases = [\n",
    "    (\"Strategic Analyst\", person_1),\n",
    "    (\"Enthusiastic Communicator\", person_2),\n",
    "    (\"Reliable Organizer\", person_3),\n",
    "    (\"Energetic Performer\", person_4),\n",
    "    (\"Insightful Idealist\", person_5),\n",
    "    (\"Innovative Debater\", person_6),\n",
    "    (\"Supportive Helper\", person_7),\n",
    "    (\"Action-Oriented Problem Solver\", person_8)\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MBTI PREDICTION TEST SUITE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, text in test_cases:\n",
    "    result = predict_mbti(text)\n",
    "    print(f\"\\nğŸ§‘ {name}\")\n",
    "    print(f\"ğŸ¯ Predicted Type: {result}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Interactive mode - paste your own text\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERACTIVE MODE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nPaste your text below (or type 'exit' to quit):\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Your text: \").strip()\n",
    "    if user_input.lower() in ['exit', 'quit', '']:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    result = predict_mbti(user_input)\n",
    "    print(f\"ğŸ¯ Predicted MBTI Type: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edadc04-83e6-4512-9e26-c28c4ff85119",
   "metadata": {},
   "source": [
    "## Expected output Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c31b93f-8cbf-415c-9ca9-a84d43634695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6f806bf065471e8aa3886ef57c9da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing prediction...\n",
      "======================================================================\n",
      "\n",
      "--- RAW REPLY ---\n",
      "You are an MBTI expert. Return ONLY the 4-letter type. Analyze:\n",
      "\n",
      "\"\n",
      "Name: Al Amin\n",
      "Headline: Senior Software Engineer | AI Enthusiast\n",
      "Location: Dhaka, Bangladesh\n",
      "About: Passionate about building scalable systems and mentoring juniors. Love diving deep into algorithms and optimizing performance.\n",
      "Posts: Just deployed a new RAG pipeline â€” 40% faster retrieval! | Teaching a workshop on LoRA fine-tuning next week. Excited to share knowledge! | Spent weekend reading papers on efficient transformers.\n",
      "\" ENFJ\n",
      "--- END RAW ---\n",
      "\n",
      "| MBTI Type | Key Traits            | Description                         | Fit for Dhaka Business Context                     |\n",
      "|-----------|-----------------------|-------------------------------------|----------------------------------------------------|\n",
      "| **ENFJ**    | Charismatic, mentoring             | Attuned to others' emotions             | Great for sales & partnerships                                         |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1. Load model + LoRA (4-bit)\n",
    "# --------------------------------------------------------------\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "adapter_name = \"alam1n/phi3-mbti-lora\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapter_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. MBTI Table (16 types)\n",
    "# --------------------------------------------------------------\n",
    "MBTI_TABLE = {\n",
    "    \"INTJ\": (\"Strategic, independent\", \"Original minds with drive for goals\", \"Strong in planning roles, e.g., tech startups\"),\n",
    "    \"INTP\": (\"Analytical, inventive\", \"Seek logical explanations\", \"Great for R&D in Dhaka AI labs\"),\n",
    "    \"ENTJ\": (\"Decisive, leader\", \"Assume leadership readily\", \"Ideal for scaling startups\"),\n",
    "    \"ENTP\": (\"Innovative, debating\", \"Resourceful in new challenges\", \"Perfect for product ideation\"),\n",
    "    \"INFJ\": (\"Insightful, idealistic\", \"Seek meaning & connection\", \"Excellent in HR / culture building\"),\n",
    "    \"INFP\": (\"Creative, empathetic\", \"Loyal to values\", \"Strong in content & community\"),\n",
    "    \"ENFJ\": (\"Charismatic, mentoring\", \"Attuned to others' emotions\", \"Great for sales & partnerships\"),\n",
    "    \"ENFP\": (\"Enthusiastic, imaginative\", \"See possibilities\", \"Best for marketing & outreach\"),\n",
    "    \"ISTJ\": (\"Dependable, thorough\", \"Responsible & detail-oriented\", \"Core for operations & compliance\"),\n",
    "    \"ISFJ\": (\"Conscientious, supportive\", \"Committed to obligations\", \"Perfect for customer success\"),\n",
    "    \"ESTJ\": (\"Organized, decisive\", \"Implement decisions fast\", \"Strong in project management\"),\n",
    "    \"ESFJ\": (\"Harmonious, cooperative\", \"Want harmony\", \"Ideal for team coordination\"),\n",
    "    \"ISTP\": (\"Practical, adaptable\", \"Quick problem-solver\", \"Excellent in DevOps / troubleshooting\"),\n",
    "    \"ISFP\": (\"Artistic, flexible\", \"Enjoy present moment\", \"Great for UI/UX design\"),\n",
    "    \"ESTP\": (\"Action-oriented, pragmatic\", \"Focus on immediate results\", \"Perfect for field sales\"),\n",
    "    \"ESFP\": (\"Outgoing, spontaneous\", \"Enjoy working with people\", \"Best for events & community\"),\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. Correct inference function (EXACT prompt used in training)\n",
    "# --------------------------------------------------------------\n",
    "# --------------------------------------------------------------\n",
    "# 3. Inference â€“ robust cleaning + table output\n",
    "# --------------------------------------------------------------\n",
    "import re\n",
    "\n",
    "def predict_mbti_and_table(person_text: str) -> str:\n",
    "    if len(person_text) > 3200:\n",
    "        person_text = person_text[:3200] + \"...\"\n",
    "\n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are an MBTI expert. Return ONLY the 4-letter type.<|end|>\n",
    "<|user|>\n",
    "Analyze:\n",
    "\n",
    "\"{person_text}\"<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=8,\n",
    "            temperature=None,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    reply = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"\\n--- RAW REPLY ---\")\n",
    "    print(reply)\n",
    "    print(\"--- END RAW ---\\n\")\n",
    "\n",
    "    # âœ… NEW: Extract MBTI using regex\n",
    "    match = re.findall(r'\\b[IE][NS][FT][JP]\\b', reply.upper())\n",
    "    mbti = match[-1] if match else \"UNKNOWN\"\n",
    "\n",
    "    traits, desc, context = MBTI_TABLE.get(mbti, (\"â€“\", \"â€“\", \"â€“\"))\n",
    "\n",
    "    table = f\"\"\"\n",
    "| MBTI Type | Key Traits            | Description                         | Fit for Dhaka Business Context                     |\n",
    "|-----------|-----------------------|-------------------------------------|----------------------------------------------------|\n",
    "| **{mbti}**    | {traits}             | {desc}             | {context}                                         |\n",
    "\"\"\"\n",
    "    return table.strip()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4. Test with real data\n",
    "# --------------------------------------------------------------\n",
    "person_text = \"\"\"\n",
    "Name: Al Amin\n",
    "Headline: Senior Software Engineer | AI Enthusiast\n",
    "Location: Dhaka, Bangladesh\n",
    "About: Passionate about building scalable systems and mentoring juniors. Love diving deep into algorithms and optimizing performance.\n",
    "Posts: Just deployed a new RAG pipeline â€” 40% faster retrieval! | Teaching a workshop on LoRA fine-tuning next week. Excited to share knowledge! | Spent weekend reading papers on efficient transformers.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing prediction...\")\n",
    "print(\"=\"*70)\n",
    "print(predict_mbti_and_table(person_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160d230-fe0c-4a69-a3b5-11e935bc40e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
